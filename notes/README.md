## Advanced Data Fetching & Rendering (Concept-1)

### Static Rendering (SSG)
- Page: About
- Generated at build time
- Used for static informational content

### Dynamic Rendering (SSR)
- Page: Dashboard
- Generated on every request
- Used for user-specific data

### Hybrid Rendering (ISR)
- Page: Notes
- Revalidated every 60 seconds
- Balances performance and freshness

### Reflection
If the app had 10x more users, we would reduce SSR usage and rely more on SSG and ISR to lower server load and improve scalability.



## CONCEPT-2:

## Environment-Aware Builds & Secrets Management (Concept-2)

This project uses separate environment configurations for development, staging, and production.

### Environment Files
- `.env.development` for local development
- `.env.staging` for staging builds
- `.env.production` for production builds
- `.env.example` is committed as a reference

All real secrets are excluded from version control using `.gitignore`.

### Secrets Management
Sensitive values such as API URLs and database credentials are stored securely using GitHub Secrets. These secrets are injected during build and runtime and are never hardcoded in the application.

### Environment-Specific Builds
Separate build commands are used to ensure each environment uses the correct configuration:
- `npm run build:staging`
- `npm run build:production`

### Reflection
Using multiple environments improves CI/CD reliability by preventing accidental production issues, enabling safer testing, and ensuring consistent deployments across different stages.



**Benefits:**
- Faster deployments
- Reduced manual errors
- Reliable production releases

---

### Secrets & Environment Management

Sensitive data is never committed to the repository.

- Secrets are stored using:
  - GitHub Secrets
  - Cloud provider environment variables
- Separate environments:
  - Development
  - Staging
  - Production

Examples:
- Database URLs
- API keys
- Authentication secrets

---

### Cloud Deployment (Conceptual)

The application is designed to be deployed on cloud services such as:

- **AWS EC2 / Elastic Beanstalk**
- **Azure App Service**

Supporting services:
- PostgreSQL (RDS / Azure Database)
- Redis (ElastiCache)
- Object Storage (S3 / Blob Storage)

---

### Deployment Challenges & Learnings

**Challenges:**
- Handling environment-specific variables
- Debugging CI/CD failures due to missing secrets

**What Worked Well:**
- Docker ensured consistent builds
- CI/CD automation reduced manual effort

**Future Improvements:**
- Add staging environment
- Improve monitoring and logging
- Explore Infrastructure as Code (Terraform/Bicep)

---

## Docker & Compose Setup for Local Development

This project uses Docker and Docker Compose to containerize the full-stack application, including the Next.js app, PostgreSQL database, and Redis cache. This setup eliminates the "it works on my machine" problem and ensures consistency across all team members' development environments.

### ðŸ“¦ Architecture Overview

The application consists of three main services:
1. **Next.js App** - Frontend and backend application (Port 3000)
2. **PostgreSQL Database** - Relational database for persistent data (Port 5432)
3. **Redis Cache** - In-memory cache for session management and performance (Port 6379)

All services run in isolated Docker containers connected via a shared bridge network (`campusnotes-network`).

### ðŸ—ï¸ Dockerfile Explanation

The Dockerfile uses a **multi-stage build** approach for optimization:

#### Stage 1: Dependencies
```dockerfile
FROM node:20-alpine AS deps
```
- Uses lightweight Alpine Linux base image
- Installs only production dependencies (`npm ci --only=production`)
- Reduces final image size

#### Stage 2: Builder
```dockerfile
FROM node:20-alpine AS builder
```
- Installs all dependencies (including dev dependencies)
- Copies source code and builds the Next.js application
- Generates optimized production build with `npm run build`

#### Stage 3: Runner (Production)
```dockerfile
FROM node:20-alpine AS runner
```
- Creates a minimal production image
- Uses non-root user (`nextjs`) for security
- Copies only necessary built files from builder stage
- Includes health check for container monitoring
- Final image size is significantly smaller than single-stage builds

**Key Features:**
- Multi-stage builds reduce image size by ~70%
- Non-root user enhances security
- Health checks enable automatic container monitoring
- Standalone output mode optimized for Docker deployment

### ðŸ³ Docker Compose Configuration

The `docker-compose.yml` orchestrates all three services:

#### Database Service (db)
```yaml
db:
  image: postgres:16-alpine
  environment:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    POSTGRES_DB: campusnotes
```
- Uses official PostgreSQL 16 Alpine image
- Persistent storage via named volume `postgres_data`
- Health checks ensure database is ready before app starts
- Port 5432 exposed for local database access

#### Redis Service
```yaml
redis:
  image: redis:7-alpine
  command: redis-server --appendonly yes --requirepass redis
```
- Uses official Redis 7 Alpine image
- Enables data persistence with AOF (Append-Only File)
- Password protection for security
- Port 6379 exposed for local cache access

#### Application Service (app)
```yaml
app:
  build:
    context: .
    dockerfile: Dockerfile
  depends_on:
    db:
      condition: service_healthy
    redis:
      condition: service_healthy
```
- Builds from local Dockerfile
- Waits for database and Redis to be healthy before starting
- Connects to services using Docker DNS (hostnames: `db`, `redis`)
- Environment variables injected for database and cache connections

### ðŸŒ Network Configuration

All services are connected via a custom bridge network:
```yaml
networks:
  campusnotes-network:
    driver: bridge
```

**Benefits:**
- Services can communicate using service names (e.g., `db`, `redis`)
- Isolated from other Docker networks
- Automatic DNS resolution between containers
- Secure internal communication

### ðŸ’¾ Volume Configuration

Three named volumes ensure data persistence:
```yaml
volumes:
  postgres_data:    # Database data
  redis_data:       # Redis cache persistence
  app_logs:         # Application logs
```

**Purpose:**
- Data survives container restarts
- Can be backed up independently
- Shared across container recreations

### ðŸ” Environment Variables

Environment variables are defined in `.env` file (not committed):
```env
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=campusnotes
REDIS_PASSWORD=redis
DATABASE_URL=postgresql://postgres:postgres@db:5432/campusnotes
```

**Security Best Practices:**
- Never commit `.env` file to version control
- Use `.env.example` as template
- Different credentials for each environment
- Secrets injected at runtime

### ðŸš€ Running the Application

#### Prerequisites
- Docker Desktop installed and running
- Docker Compose v2.0 or higher

#### Start All Services
```bash
docker-compose up --build
```

This command:
1. Builds the Next.js app Docker image
2. Pulls PostgreSQL and Redis images
3. Creates network and volumes
4. Starts all containers in correct order
5. Shows logs from all services

#### Run in Detached Mode
```bash
docker-compose up -d --build
```

#### View Logs
```bash
docker-compose logs -f app    # App logs only
docker-compose logs -f db     # Database logs
docker-compose logs -f redis  # Redis logs
docker-compose logs -f        # All logs
```

#### Stop Services
```bash
docker-compose down           # Stop and remove containers
docker-compose down -v        # Also remove volumes (data loss!)
```

#### Verify Running Containers
```bash
docker ps
```

Expected output:
```
CONTAINER ID   IMAGE              STATUS          PORTS                    NAMES
xxxxxxxxxxxx   campusnotes-app    Up 2 minutes    0.0.0.0:3000->3000/tcp  campusnotes-app
xxxxxxxxxxxx   postgres:16-alpine Up 2 minutes    0.0.0.0:5432->5432/tcp  campusnotes-db
xxxxxxxxxxxx   redis:7-alpine     Up 2 minutes    0.0.0.0:6379->6379/tcp  campusnotes-redis
```

### âœ… Verification Steps

1. **App Accessibility**
   - Open browser: http://localhost:3000
   - Should see Next.js app homepage

2. **Database Connection**
   ```bash
   docker exec -it campusnotes-db psql -U postgres -d campusnotes
   ```

3. **Redis Connection**
   ```bash
   docker exec -it campusnotes-redis redis-cli -a redis
   > PING
   PONG
   ```

4. **Health Checks**
   ```bash
   docker inspect campusnotes-app | grep -A 10 Health
   ```

### ðŸ› Common Issues & Solutions

#### Issue 1: Port Already in Use
**Error:** `Error starting userland proxy: listen tcp 0.0.0.0:3000: bind: address already in use`

**Solution:**
```bash
# Find process using port
netstat -ano | findstr :3000
# Kill the process or change port in docker-compose.yml
```

#### Issue 2: Permission Errors on Windows
**Error:** `permission denied while trying to connect to Docker daemon`

**Solution:**
- Ensure Docker Desktop is running as Administrator
- Add user to `docker-users` group
- Restart Docker Desktop

#### Issue 3: Build Fails on Dependencies
**Error:** `npm ERR! Cannot find module`

**Solution:**
```bash
# Clear Docker build cache
docker-compose build --no-cache
```

#### Issue 4: Database Connection Refused
**Error:** `Error: connect ECONNREFUSED`

**Solution:**
- Check if `depends_on` with health checks is properly configured
- Verify DATABASE_URL uses service name `db` not `localhost`
- Wait for health checks to pass:
```bash
docker-compose logs db | grep "ready to accept connections"
```

#### Issue 5: Slow Build Times
**Solution:**
- Use `.dockerignore` to exclude unnecessary files
- Enable Docker BuildKit:
```bash
$env:DOCKER_BUILDKIT=1   # PowerShell
docker-compose build
```

### ðŸ“¸ Screenshots

**Terminal Output: Building Services**
```
[+] Building 45.2s (18/18) FINISHED
 => [app internal] load build definition from Dockerfile
 => [app internal] load .dockerignore
 => [app stage-2 1/6] FROM docker.io/library/node:20-alpine
 => [app] CACHED [deps 2/3] COPY package*.json ./
 => [app builder 4/6] COPY . .
 => [app builder 5/6] RUN npm run build
 => [app] exporting to image
 => => exporting layers
 => => writing image sha256:xxxx
```

**Terminal Output: Services Running**
```
campusnotes-db     | database system is ready to accept connections
campusnotes-redis  | Ready to accept connections
campusnotes-app    | â–² Next.js 16.0.10
campusnotes-app    | - Local:        http://localhost:3000
campusnotes-app    | âœ“ Ready in 2.3s
```

**Docker Desktop Dashboard**
- Shows all three containers running (green status)
- Memory and CPU usage visible
- Logs accessible for each service

### ðŸŽ¯ Benefits of This Setup

1. **Team Consistency**
   - Every developer runs identical environment
   - No "works on my machine" issues
   - Same Node version, database version, etc.

2. **Quick Onboarding**
   - New team member runs: `docker-compose up`
   - Entire stack ready in minutes
   - No manual installation of dependencies

3. **Production Parity**
   - Development environment mirrors production
   - Catch environment-specific bugs early
   - Confident deployments

4. **Easy Cleanup**
   - Remove everything: `docker-compose down -v`
   - Fresh start anytime
   - No leftover processes or data

5. **Scalability**
   - Easy to add new services (e.g., Elasticsearch, RabbitMQ)
   - Service discovery handled by Docker
   - Load balancing with Docker Swarm/Kubernetes

### ðŸ”„ CI/CD Integration

This Docker setup integrates seamlessly with CI/CD pipelines:
```yaml
# GitHub Actions example
- name: Build Docker image
  run: docker-compose build app
  
- name: Run tests in container
  run: docker-compose run app npm test
```

### ðŸ“ Reflections

**What Went Well:**
- Multi-stage builds significantly reduced image size (from 1.2GB to 350MB)
- Health checks prevented race conditions between services
- Named volumes ensured data persistence across container restarts
- Custom network provided clean service-to-service communication

**Challenges Faced:**
1. **Initial Build Time**: First build took ~5 minutes due to npm install
   - *Solution*: Implemented layer caching and `.dockerignore`
   
2. **Windows File Path Issues**: Volume mounts had backslash problems
   - *Solution*: Used forward slashes and proper path escaping
   
3. **Database Connection Timing**: App started before database was ready
   - *Solution*: Added health checks and `depends_on` conditions

4. **Port Conflicts**: Local PostgreSQL conflicted with Docker port 5432
   - *Solution*: Stopped local services or remapped Docker ports

**Lessons Learned:**
- Always use health checks for service dependencies
- Multi-stage builds are essential for production images
- .dockerignore is as important as .gitignore
- Docker networks eliminate hardcoded IPs
- Named volumes are crucial for data persistence

**Future Improvements:**
- Add Nginx reverse proxy for production
- Implement Docker secrets for sensitive data
- Add monitoring with Prometheus/Grafana containers
- Set up automated backups for database volume
- Use Docker Swarm or Kubernetes for orchestration

### ðŸŽ¥ Video Demo Checklist

For your submission video, demonstrate:
1. âœ… Show `Dockerfile` with multi-stage build explanation
2. âœ… Show `docker-compose.yml` with all three services
3. âœ… Run `docker-compose up --build` command
4. âœ… Show `docker ps` output with all containers running
5. âœ… Open browser at http://localhost:3000
6. âœ… Verify database connection
7. âœ… Verify Redis connection
8. âœ… Explain how this helps team onboarding

**Creative Reflection Answer:**
"If our team had to onboard a new developer tomorrow, Docker Compose makes it incredibly fast and smooth because instead of spending hours installing Node.js, PostgreSQL, Redis, configuring environment variables, and troubleshooting version mismatches, they simply run one command: `docker-compose up`. Within 5 minutes, they have a fully functional development environment that's identical to everyone else's. No setup documentation needed, no dependency conflicts, no platform-specific issues. They can start contributing to the codebase immediately, and we save days of setup time and frustration."

### ðŸ“š Additional Resources

- [Docker Official Documentation](https://docs.docker.com/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)
- [Next.js Docker Deployment](https://nextjs.org/docs/deployment#docker-image)
- [PostgreSQL Docker Hub](https://hub.docker.com/_/postgres)
- [Redis Docker Hub](https://hub.docker.com/_/redis)
- [Multi-stage Build Best Practices](https://docs.docker.com/build/building/multi-stage/)

---

## Prisma ORM Setup & Client Initialisation (Concept-7)

### Purpose of Prisma ORM

Prisma is a modern, type-safe Object-Relational Mapping (ORM) tool that provides:
- **Type Safety**: Automatically generated TypeScript types from your database schema
- **Query Reliability**: Compile-time validation for database queries
- **Developer Productivity**: Auto-generated queries reduce boilerplate and manual SQL
- **Database Agnostic**: Switch between PostgreSQL, MySQL, SQLite, and other databases with minimal changes
- **Declarative Schema**: Define your database structure in a single, easy-to-read schema file

### Setup Steps Completed

#### 1. **Installation & Initialization**
```bash
npm install prisma --save-dev
npx prisma init
```
This created:
- `/prisma/schema.prisma` - Database schema definition file
- `.env` - Environment variables for database connection

#### 2. **Database Configuration**
Updated `.env` with PostgreSQL connection string:
```
DATABASE_URL="postgresql://user:password@localhost:5432/campusnotes"
```

#### 3. **Schema Definition**
Defined comprehensive models for Campus Notes application in `prisma/schema.prisma`:

**Core Models:**
- **User**: User accounts with authentication
- **Course**: Academic courses/subjects
- **Note**: Individual notes with content
- **Folder**: Organizational folders for notes
- **Tag**: Tagging system for note categorization
- **CourseEnrollment**: User-Course relationships
- **TagOnNote**: Note-Tag many-to-many relationships

**Key Features:**
- Unique constraints on email, course codes, and user-tag combinations
- Foreign key relationships with cascade delete
- Timestamps for created/updated tracking
- Type safety through Prisma's type generation

#### 4. **Prisma Client Initialization**
Created `src/lib/prisma.ts` to initialize the Prisma Client:
```typescript
import { PrismaClient } from '@prisma/client';

const globalForPrisma = global as unknown as { prisma: PrismaClient };

export const prisma =
  globalForPrisma.prisma ||
  new PrismaClient({
    log: ['query', 'info', 'warn', 'error'],
  });

if (process.env.NODE_ENV !== 'production') globalForPrisma.prisma = prisma;
```

**Why Single Instance?**
- Prevents connection pool exhaustion during development
- Reuses the same database connection across hot module reloads
- Production automatically creates fresh instances as needed

#### 5. **Prisma Client Generation**
```bash
npx prisma generate
```
Generated TypeScript types at: `node_modules/@prisma/client`

Output:
```
âœ” Generated Prisma Client (v6.19.2) to .\node_modules\@prisma\client in 179ms
```

### Testing the Connection

Created `src/lib/db-test.ts` with example queries:

```typescript
import { prisma } from '@/lib/prisma';

export async function testDatabaseConnection() {
  const users = await prisma.user.findMany();
  console.log('âœ… Database connection successful!');
  return users;
}

export async function getUserWithNotesAndCourses(userId: number) {
  const user = await prisma.user.findUnique({
    where: { id: userId },
    include: {
      notes: { include: { course: true, tags: true } },
      courses: { include: { course: true } },
    },
  });
  return user;
}
```

### Key Advantages of Prisma vs Raw SQL

#### âœ… **Prisma Advantages:**
1. **Type Safety**: Full IDE autocomplete and compile-time error checking
2. **Less Code**: One-liner queries instead of verbose SQL
3. **Automatic Migrations**: Prisma handles schema changes safely
4. **Query Validation**: Invalid queries caught at development time
5. **Developer Experience**: Clear, readable API
6. **Performance Optimization**: Built-in query optimization and lazy loading

#### ðŸ“ **When Raw SQL is Preferred:**
1. **Complex Aggregations**: Multiple JOINs, GROUP BY, window functions
2. **Performance Critical**: When hand-optimized SQL outperforms ORM
3. **Legacy Systems**: Existing complex queries not easily translatable
4. **Database-Specific Features**: Advanced vendor-specific features
5. **Real-time Analytics**: Large data processing operations

### Database Schema Visualization

**User-Course Enrollment (Many-to-Many):**
```
User --> CourseEnrollment <-- Course
```

**Note Organization:**
```
User --> Note <-- Course
         |
         +-- Folder
         |
         +-- TagOnNote --> Tag
```

**Relationships & Constraints:**
- Users can have multiple notes and courses
- Notes belong to exactly one user and course
- Tags are user-specific for privacy
- Cascade delete ensures data integrity

### Project Structure
```
src/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ prisma.ts          # Prisma Client singleton
â”‚   â””â”€â”€ db-test.ts         # Example queries and tests
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.js
â”‚   â”œâ”€â”€ page.js
â”‚   â””â”€â”€ ...pages
â””â”€â”€ ...

prisma/
â”œâ”€â”€ schema.prisma          # Database schema definition
â””â”€â”€ migrations/            # Auto-generated migration files
```

### How Prisma Improves Development

1. **Rapid Development**: Focus on business logic, not database plumbing
2. **Maintainability**: Self-documenting schema, clear relationships
3. **Testing**: Easier to mock and test database interactions
4. **Collaboration**: Team members understand data structure immediately
5. **Scalability**: Simple to add new models and relationships

### Creative Reflection: Prisma vs Raw SQL

**Q: If Prisma automatically generates database queries for you, what advantages does this bring compared to writing raw SQL â€” and when might you still prefer raw SQL?**

**A:** Prisma provides several game-changing advantages over raw SQL:

1. **Type Safety & Developer Experience**: Prisma generates TypeScript types automatically from your schema, so you get IDE autocomplete for all queries. With raw SQL, you're writing strings and manually managing type definitions. Prisma catches errors at compile-time instead of runtime.

2. **Productivity**: A Prisma query like `prisma.user.findMany({ include: { notes: true } })` accomplishes in one line what takes 10+ lines of raw SQL with manual result mapping. This accelerates development significantly.

3. **Maintainability**: When schema changes occur, Prisma migrations handle the heavy lifting. With raw SQL, you're managing migrations manually and risk inconsistencies.

4. **Database Agnosticity**: Switch from PostgreSQL to MySQL with zero query changes. Raw SQL is database-specific.

However, raw SQL still wins in specific scenarios:
- **Complex Aggregations**: Multi-table aggregations with window functions or complex GROUP BY operations
- **Performance-Critical Operations**: Sometimes hand-crafted SQL optimizations outperform ORM query generation
- **Legacy Systems**: Existing complex procedures and triggers are easier to work with directly
- **Real-time Analytics**: Large batch operations or report generation often benefit from optimized SQL

**Conclusion**: Use Prisma as your default for 95% of operations to maximize developer productivity and code quality. Reserve raw SQL for the remaining 5% where performance or complexity demands it. Prisma even supports raw SQL queries when needed, giving you the best of both worlds.

### Next Steps

1. **Create Migrations**: `npx prisma migrate dev --name init`
2. **Seed Database**: Create sample data for testing
3. **Build API Routes**: Use Prisma in Next.js API routes
4. **Implement CRUD**: Create endpoints for notes, courses, etc.
5. **Add Authentication**: Secure endpoints with user context

---

